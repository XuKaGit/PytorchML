{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e095974a",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "## 1. Performance estimation techniques\n",
    "\n",
    "Always evaluate models as if they are predicting future data.\n",
    "<p>\n",
    "We do not have access to future data, so we pretend that some data is hidden.\n",
    "<p>\n",
    "\n",
    "Simplest way: `holdout` Randomly split data (and corresponding labels) into training and test set (e.g. 75%-25%)\n",
    "\n",
    "- Use `holdout` for very large datasets (e.g. >1.000.000 examples). Or when learners don’t always converge (e.g. deep learning)\n",
    "\n",
    "### 1.1. K-fold Cross-validation\n",
    "\n",
    "\n",
    "- 数据划分: 将数据集随机分成 k 个相同大小的子集(折,fold)\n",
    "- 模型训练与评估: 对于每一个子集, \n",
    "    - 将当前子集用作测试集, 其余 k-1 个子集合并作为训练集.\n",
    "    - 在训练集上训练模型.\n",
    "    - 在测试集上评估模型性能, 通常使用一些指标(如准确率、F1 分数等).\n",
    "- 性能汇总: 收集 k 次评估的性能指标, 计算其平均值和标准差. \n",
    "- 选择最优模型: 通过比较不同模型的平均性能指标, 选择在 k-fold 验证中表现最好的模型. \n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "```\n",
    "\n",
    "\n",
    "**Remark :** 一般情况下3/5是默认选项, 常建议用K=10\n",
    "\n",
    "\n",
    "### 1.2. Stratified K-Fold cross-validation\n",
    "\n",
    "If the data is unbalanced, some classes have only few samples -----> Stratification: **proportions** between classes are conserved in each fold\n",
    "\n",
    "- Always use stratification for classification (sklearn does this by default)\n",
    "\n",
    "\n",
    "### 1.3. LOOCV\n",
    "\n",
    "- LOOCV (Leave-one-out cross-validation)\n",
    "\n",
    "<p>\n",
    "\n",
    "- 只从可用的数据集中保留一个数据点, 并根据其余数据训练模型. 此过程对每个数据点进行迭代, 比如有n个数据点, 就要重复交叉验证n次.例如一共10个数据, 就交叉验证十次. `test_error` = 所有error的平均数.\n",
    "\n",
    "\n",
    "- **优点:**\n",
    "\n",
    "    - 适合小样本数据集\n",
    "    - 利用所有的数据点，因此偏差将很低\n",
    "\n",
    "- **缺点:**\n",
    "\n",
    "    - 重复交叉验证过程n次导致更高的执行时间.\n",
    "    - 测试模型有效性的变化大. 因为针对一个数据点进行测试, 模型的估计值受到数据点的很大影响. 如果数据点被证明是一个离群值, 它可能导致更大的变化.\n",
    "\n",
    "\n",
    "### 1.4. Shuffle-Split cross-validation\n",
    "\n",
    "Shuffles the data, samples (`train_size`) points randomly as the training set. Never use if the data is ordered (e.g. time series)\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "ss = ShuffleSplit(n_splits=5, train_size=0.8, test_size=0.2)\n",
    "for train_index, test_index in ss.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "```\n",
    "\n",
    "### 1.5. The Bootstrap (自助法)\n",
    "\n",
    "从 `n(数据集大小)` 个数据点中进行有放回抽样, 作为训练集(自助样本, `the bootstrap`)\n",
    "\n",
    "<p>\n",
    "\n",
    "On average, bootstraps include **66%** of all data points (some are duplicates).\n",
    "<p>\n",
    "\n",
    "Use the **unsampled (out-of-bootstrap)** samples as the test set\n",
    "<p>\n",
    "\n",
    "Repeat $k$ times to obtain $k$ scores. Similar to Shuffle-Split with `train_size=0.66`, `test_size=0.34` but with duplicates.\n",
    "\n",
    "\n",
    "### 1.6. Repeated cross-validation\n",
    "\n",
    "Cross-validation is still biased in that the initial split can be made in many ways.\n",
    "<p>\n",
    "Repeated, or n-times-k-fold cross-validation:\n",
    "<p>\n",
    "Shuffle data randomly, do k-fold cross-validation\n",
    "<p>\n",
    "Repeat n times, yields n times k scores\n",
    "<p>\n",
    "Unbiased, very robust, but n times more expensive\n",
    "\n",
    "### 1.7. Cross-validation with groups\n",
    "\n",
    "有时数据包含固有的组: 来自同一患者的多个样本、来自同一人的图像...... <p>\n",
    "\n",
    "同一个人的数据可能会同时出现在训练集和测试集中, 我们希望衡量模型对其他人的泛化能力, 确保每个人的数据只在训练集或测试集中, 这称为分组.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "for train_index, test_index in gkf.split(X, y, groups=groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "```\n",
    "\n",
    "### 1.8. Time series\n",
    "\n",
    "When the data is ordered, random test sets are not a good idea.<p>\n",
    "\n",
    "**Test-then-train (prequential evaluation)**: Every new sample is evaluated only once, then added to the **training set**. Can also be done in batches (of n samples at a time).\n",
    "\n",
    "- `TimeSeriesSplit`: 在第 k 次切分中, 前 k 个折作为训练集, 第 (k+1) 个折作为测试集. 随着 k 的增加, 训练集也会逐渐增大. 假设有一个时间序列数据集, 数据点为 1 到 10, `TimeSeriesSplit` 可能会产生以下切分:\n",
    "\n",
    "    - 1-3 训练集 | 4 测试集\n",
    "    - 1-4 训练集 | 5 测试集\n",
    "    - 1-5 训练集 | 6 测试集\n",
    "    - 1-6 训练集 | 7 测试集\n",
    "    - 1-7 训练集 | 8 测试集\n",
    "    - 1-8 训练集 | 9 测试集\n",
    "    - 1-9 训练集 | 10 测试集\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95f06a",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metrics for Classification\n",
    "\n",
    "### 2.1 Binary Classification\n",
    "\n",
    "2 different kind of errors:\n",
    "\n",
    "- **False Positive (type I error)**: model predicts positive while true label is negative\n",
    "\n",
    "- **False Negative (type II error)**: model predicts negative while true label is positive\n",
    "\n",
    "- We can represent all predictions (correct and incorrect) in a **confusion matrix**\n",
    "    - n by n array (n is the number of classes)\n",
    "    - Rows correspond to true classes, columns to predicted classes\n",
    "    - Count how often samples belonging to a class C are classified as C or any other class.\n",
    "    - For binary classification, we label these true negative (TN), true positive (TP), false negative (FN), false positive (FP)\n",
    "\n",
    "| | Predicted Neg | Predicted Pos |\n",
    "|-|-|-|\n",
    "| Actual Neg | TN | FP |\n",
    "| Actual Pos | FN | TP |\n",
    "\n",
    "\n",
    "\n",
    "#### Predictive accuracy\n",
    "\n",
    "- Accuracy can be computed based on the confusion matrix\n",
    "- Not useful if the dataset is very imbalanced\n",
    "    - E.g. credit card fraud: is 99.99% accuracy good enough? \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Precision\n",
    "- Use when the goal is to limit FPs\n",
    "    - Clinical trails: you only want to test drugs that really work\n",
    "    - Search engines: you want to avoid bad search results\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "\\end{equation}\n",
    "\n",
    "#### Recall\n",
    "- Use when the goal is to limit FNs\n",
    "    - Cancer diagnosis: you don't want to miss a serious disease\n",
    "    - Search engines: You don't want to omit important hits\n",
    "- Also know as sensitivity, hit rate, true positive rate (TPR)\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "\\end{equation}\n",
    "\n",
    "#### F1-score\n",
    "- Trades off precision and recall:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{equation}\n",
    "\n",
    "### 2.2. Multi-class classification\n",
    "\n",
    "- Train models _per class_ : one class viewed as positive, other(s) als negative, then average\n",
    "    - micro-averaging: count total TP, FP, TN, FN (every sample equally important)\n",
    "        - micro-precision, micro-recall, micro-F1, accuracy are all the same\n",
    "        $$\\text{Precision:} \\frac{\\sum_{c=1}^C\\text{TP}_c}{\\sum_{c=1}^C\\text{TP}_c + \\sum_{c=1}^C\\text{FP}_c} \\xrightarrow{c=2} \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n",
    "        \n",
    "    - macro-averaging: average of scores $R(y_c,\\hat{y_c})$ obtained on each class\n",
    "        - Preferable for imbalanced classes (if all classes are equally important)\n",
    "        - macro-averaged recall is also called _balanced accuracy_ \n",
    "         $$\\frac{1}{C} \\sum_{c=1}^C R(y_c,\\hat{y_c})$$\n",
    "    - weighted averaging ($w_c$: ratio of examples of class $c$, aka support): $\\sum_{c=1}^C w_c R(y_c,\\hat{y_c})$\n",
    "\n",
    "\n",
    "### 2.3. Other useful classification metrics\n",
    "- Cohen's Kappa\n",
    "    - Measures 'agreement' between different models (aka inter-rater agreement)\n",
    "    - To evaluate a single model, compare it against a model that does random guessing\n",
    "        - Similar to accuracy, but taking into account the possibility of predicting the right class by chance\n",
    "    - Can be weighted: different misclassifications given different weights\n",
    "    - 1: perfect prediction, 0: random prediction, negative: worse than random\n",
    "    - With $p_0$ = accuracy, and $p_e$ = accuracy of random classifier:\n",
    "        $$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\n",
    "- Matthews correlation coefficient\n",
    "    - Corrects for imbalanced data, alternative for balanced accuracy or AUROC\n",
    "    - 1: perfect prediction, 0: random prediction, -1: inverse prediction\n",
    "        $$MCC = \\frac{tp \\times tn - fp \\times fn}{\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}$$\n",
    "\n",
    "### 2.4. ROC曲线和AUC值\n",
    "\n",
    "- Trade off _true positive rate_ $\\textit{TPR}= \\frac{TP}{TP + FN}$ with _false positive rate_ $\\textit{FPR} = \\frac{FP}{FP + TN}$\n",
    "- Plotting TPR against FPR _for all possible thresholds_ yields a _Receiver Operating Characteristics curve_\n",
    "    - Change the treshold until you find a sweet spot in the TPR-FPR trade-off\n",
    "    - Lower thresholds yield higher TPR (recall), higher FPR, and vice versa\n",
    "\n",
    "- AUC: Area under curve: 值越接近1说明模型效果越好\n",
    "\n",
    "### 2.5. Cost-sensitive classification (dealing with imbalance)\n",
    "\n",
    "In the real world, different kinds of misclassification can have different costs\n",
    "\n",
    "- Misclassifying certain classes can be more costly than others\n",
    "\n",
    "- Misclassifying certain samples can be more costly than others\n",
    "\n",
    "Cost-sensitive resampling: **resample (or reweight)** the data to represent real-world expectations\n",
    "\n",
    "- oversample minority classes (or undersample majority) to ‘correct’ imbalance\n",
    "\n",
    "- increase weight of misclassified samples (e.g. in boosting)\n",
    "\n",
    "- decrease weight of misclassified (noisy) samples (e.g. in model compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26c29d",
   "metadata": {},
   "source": [
    "## 3. Bias-Variance Trade-off\n",
    "\n",
    "**Motivation:** To characterize the learning behavior from statistical perspective\n",
    "\n",
    "**泛化能力**: 模型在不同样本集(真实分布相同)上的预测效果是否一致, 也即模型是否具备可推广性. \n",
    "\n",
    "- 见[机器学习基本概念：偏差、方差、误差和噪声](https://zhuanlan.zhihu.com/p/23090568344)\n",
    "- 见[DDA3020-LectureNotes](../DDA3020/L11_Bias_Variance_Decomposition.pdf)\n",
    "\n",
    "## 结论\n",
    "理解偏差和方差对于提升模型性能非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6044a5",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [交叉验证方法汇总](https://blog.csdn.net/WHYbeHERE/article/details/108192957)\n",
    "- [Cross-Validation（交叉验证）详解](https://zhuanlan.zhihu.com/p/24825503)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
