{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T18:36:15.790189Z",
     "start_time": "2025-03-11T18:36:12.944821Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `nn.Module`\n",
    "\n",
    "```python\n",
    "\n",
    "CLASS torch.nn.Module  # 为所有神经网络提供基本骨架, 搭建的 Model都必须继承该类\n",
    "```\n",
    "\n",
    "模板:\n",
    "\n",
    "```python\n",
    "\n",
    "class MyNet(nn.Module):   #搭建的神经网络 MyNet 继承了 Module类\n",
    "    def __init__(self):   \n",
    "        super(Model, self).__init__()   #必须要这一步，调用父类的初始化函数\n",
    "        self.conv1 = nn.Conv2d(3, 3, 5)\n",
    "        self.conv2 = nn.Conv2d(3, 3, 5)\n",
    " \n",
    "    def forward(self, x):   #前向传播（为输入和输出中间的处理过程）, x为输入\n",
    "        x = F.relu(self.conv1(x))   \n",
    "        return F.relu(self.conv2(x))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 卷积,池化,非线性激活,全连接\n",
    "\n",
    "### 2.1.  2D卷积,`nn.Conv2d`和`F.conv2d`\n",
    "\n",
    "- `nn.Conv2d`(类式接口)\n",
    "- `F.conv2d`(函数式接口): 函数式的更加low-level一些，如果不需要做特别复杂的配置只要用**类式接口**就够了\n",
    "- 可以这样理解：`nn.Conv2d`是[2D卷积层]，而`F.conv2d`是[2D卷积操作]\n",
    "\n",
    "---------------------------------------------------------------  <p>\n",
    "\n",
    "**`nn.Conv2d()`的参数:** <p>\n",
    "`nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,padding_mode='zeros')`\n",
    "- in_channels: 输入图像的通道数\n",
    "- out_channels: 卷积生成的通道数\n",
    "- kernel_size:  指卷积核的长宽,如为3则卷积核是3乘3的\n",
    "\n",
    "**`F.conv2d()`的参数:** <p>\n",
    "`nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor`\n",
    "\n",
    "- **input:** input tensor of shape (batchszie , in_channels , Height , Width)\n",
    "- **weight:** weight tensor of shape (out_channels , in_channels/groups , H , W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3487, 0.5030, 0.6218, 0.6449, 0.0802],\n",
      "          [0.1641, 0.5387, 0.9858, 0.6082, 0.9344],\n",
      "          [0.9847, 0.7799, 0.6181, 0.5815, 0.5670],\n",
      "          [0.9748, 0.2992, 0.3843, 0.8859, 0.4536],\n",
      "          [0.6485, 0.6504, 0.2218, 0.2132, 0.3070]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[-0.2211, -0.0562, -0.2652],\n",
      "          [-0.1037,  0.2803,  0.2123],\n",
      "          [-0.2866,  0.0138,  0.0752]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0688,  0.0723, -0.2975],\n",
      "          [ 0.0280, -0.0901,  0.0888],\n",
      "          [-0.0455,  0.2577,  0.1442]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1979,  0.0342, -0.2526],\n",
      "          [ 0.0367, -0.1139, -0.3222],\n",
      "          [ 0.0890, -0.3165, -0.0681]]]], requires_grad=True)\n",
      "---------------------------------------\n",
      "Parameter containing:\n",
      "tensor([ 0.0866, -0.2517, -0.1145], requires_grad=True)\n",
      "---------------------------------------\n",
      "torch.Size([1, 3, 3, 3])\n",
      "torch.Size([1, 3, 3, 3])\n",
      "---------------------------------------\n",
      "The output of nn.conv2d() is:\n",
      "tensor([[[[5.2646, 5.5551, 4.4175],\n",
      "          [5.2560, 4.7192, 5.3725],\n",
      "          [5.4092, 4.5045, 3.8906]]]]), with shape torch.Size([1, 1, 3, 3])\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"-----------nn.Conv2d()------------------\"\"\"\n",
    "\n",
    "# 输入图像的通道数 =1 , 卷积核的种类数=3\n",
    "conv_layer = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# 样本数=1,通道数=1,图像的shape是5乘5的\n",
    "input = torch.rand(1, 1, 5, 5)\n",
    "\n",
    "conv_out = conv_layer.forward(input)\n",
    "conv_out1 = conv_layer(input)   # 更常用\n",
    "\n",
    "# 得到的还是1张图片,因为用了3种kernel所以输出的通道数变成3了\n",
    "print(conv_layer.weight)\n",
    "print(\"---------------------------------------\")\n",
    "print(conv_layer.bias)\n",
    "print(\"---------------------------------------\")\n",
    "print(conv_out.shape)  # torch.Size([1, 3, 3, 3])\n",
    "print(conv_out1.shape)\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "\n",
    "\"\"\"-----------F.conv2d()------------------\"\"\"\n",
    "\n",
    "kernel = torch.tensor([[1,2,1],\n",
    "                       [0,1,0],\n",
    "                       [2,1,0]],dtype = torch.float32 )\n",
    "\n",
    "kernel = torch.reshape(kernel,(1,1,3,3))\n",
    "\n",
    "output = F.conv2d(input,kernel)\n",
    "print(\"The output of nn.conv2d() is:\")\n",
    "print(f'{output}, with shape {output.shape}')\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 池化层\n",
    "\n",
    "- **池化:** 池化层是卷积神经网络中用于减小参数数量和计算复杂度的常用技术。池化层通过在输入特征图上滑动一个固定大小的窗口，并计算窗口内元素的最大值、最小值或平均值，从而生成一个新的特征图。\n",
    "\n",
    "- **Pool** Vs **Conv**\n",
    "\n",
    "    - **池化与卷积的共同点:** 池化操作也是原图像矩阵与一个固定形状的窗口(kernel))进行计算，并输出特征图的一种计算方式;\n",
    "\n",
    "    - **池化与卷积的不同点:** 卷积操作的卷积核是有数据(权重)的，而池化直接计算池化窗口内的原始数据，这个计算过程可以是选择最大值、选择最小值或计算平均值，分别对应：最大池化、最小池化和平均池化。在实际使用中最大池化是应用最广泛的池化方法。\n",
    "\n",
    "\n",
    "- **池化种类**\n",
    "    - MaxPool：最大池化(下采样): 提取出特定窗口的最大数据. \n",
    "    - AvgPool：平均池化\n",
    "    - AdaptiveMaxPool2d：自适应最大池化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9171, 0.9558, 0.9952],\n",
      "          [0.9171, 0.9558, 0.9558],\n",
      "          [0.8920, 0.9558, 0.9558]]]])\n"
     ]
    }
   ],
   "source": [
    "maxpool_layer = nn.MaxPool2d(kernel_size=3, stride=1, padding=0)\n",
    "maxpool_output = maxpool_layer(input)   # input-shape: (1,1,5,5)\n",
    "print(maxpool_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. 非线性激活函数\n",
    "\n",
    "- `Sigmod()`: **$$\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.$$**\n",
    "              $$\\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right).$$\n",
    "其优点主要是连续，平滑便于求导. 但是其的缺点也很致命:梯度消失问题,当x>2或x<2时Sigmod输出趋于平滑，导致梯度减小,权重和偏置更新过慢导致网络不更新; 非零均值特性:会使训练震荡达不到最优解，使收敛变慢; 导数计算复杂，影响速度\n",
    "\n",
    "- `Tanh()`: Tanh主要解决了Sigmod非零均值特性的问题，但是其还是存在计算复杂和梯度消失的问题.\n",
    "**$$\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$**\n",
    "$$\\frac{d}{dx} \\operatorname{tanh}(x) = 1 - \\operatorname{tanh}^2(x).$$\n",
    "\n",
    "- `ReLU()`:  **$$\\operatorname{ReLU}(x) = \\max(x, 0).$$**\n",
    "主要优点有:大于0时，其导数恒为1，不会存在梯度消失的问题; 计算速度非常快，只需要判断 x 是大于0还是小于0; 收敛速度远远快于前面的 Sigmoid 和 Tanh函数。\n",
    "但是ReLu也是有着缺陷的: 非零均值特性; x<0时，输出恒为0, 会使某些神经元永远不会被激活，进而导致参数永远不会更新\n",
    "\n",
    "- `pReLU`: **$$\\operatorname{pReLU}(x) = \\max(0, x) + \\alpha \\min(0, x).$$**\n",
    "- `Leaky ReLU`: **$$\\operatorname{Leaky ReLU}(x) = \\max(0.1x, x).$$**\n",
    "\n",
    "- `Softmax()`: **$$\\operatorname{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j} \\exp(x_j)}.$$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1294, 0.4077, 0.0000, 0.0000, 0.0000],\n",
      "          [1.2515, 0.0000, 4.2310, 4.4363, 0.0000],\n",
      "          [0.0000, 2.1058, 0.0000, 0.0000, 0.0000],\n",
      "          [1.1504, 0.0000, 0.0000, 0.6268, 0.0000],\n",
      "          [0.0000, 3.3425, 4.4721, 2.8028, 1.0364]]]])\n",
      "---------------------------------------\n",
      "tensor([[[[0.5323, 0.6005, 0.0428, 0.1636, 0.2994],\n",
      "          [0.7776, 0.0161, 0.9857, 0.9883, 0.0817],\n",
      "          [0.0351, 0.8915, 0.1776, 0.0550, 0.0082],\n",
      "          [0.7596, 0.4569, 0.0108, 0.6518, 0.0483],\n",
      "          [0.1298, 0.9659, 0.9887, 0.9428, 0.7382]]]])\n",
      "---------------------------------------\n",
      "tensor([[[[3.4395e-01, 4.5432e-01, 1.3499e-02, 5.9111e-02, 1.2912e-01],\n",
      "          [2.2286e-02, 1.0406e-04, 4.3856e-01, 5.3849e-01, 5.6688e-04],\n",
      "          [4.2658e-03, 9.6264e-01, 2.5301e-02, 6.8265e-03, 9.6661e-04],\n",
      "          [5.3241e-01, 1.4180e-01, 1.8449e-03, 3.1540e-01, 8.5454e-03],\n",
      "          [1.1026e-03, 2.0911e-01, 6.4706e-01, 1.2189e-01, 2.0837e-02]]]])\n"
     ]
    }
   ],
   "source": [
    "a = -5 # lowerlimit\n",
    "b = 5 # upperlimit\n",
    "input = a + (b - a) * torch.rand(1, 1, 5, 5)\n",
    "\n",
    "relu = nn.ReLU(inplace=False) # 当 inplace=False 时，ReLU 会创建一个新的输出 Tensor。输入 Tensor 的值不会被修改\n",
    "relu_output = relu(input)\n",
    "print(relu_output) # 小于0的都被截断为0\n",
    "print(\"---------------------------------------\")\n",
    "sigmod = nn.Sigmoid()\n",
    "sig_output = sigmod(input)\n",
    "print(sig_output)\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "softmax = nn.Softmax(dim=3) # dim 参数指定了 Softmax 操作沿着哪个维度进行\n",
    "softmax_output = softmax(input)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. 线性连接层\n",
    "\n",
    "`torch.nn.Linear(in_features, out_features, bias=True # 是否包含偏置)`\n",
    "\n",
    "- **in_features** = i :  输入的神经元个数\n",
    "- **out_features** = o : 输出神经元个数\n",
    "- **bias**: 是否包含偏置\n",
    "\n",
    "**Linear** 其实就是对输入 $X_{n \\times i}$ **执行了一个线性变换**，即：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    Y_{n \\times o} & = X_{n \\times i} W_{i \\times o} + b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 $W$ 是模型想要学习的参数，$W$ 的维度为 $W_{i \\times o}$，$b$ 是 $o$ 维的向量偏置，$n$ 为输入向量的行数（例如，你想一次输入 10 个样本，即 `batch_size` 为 10，n=10），$i$ 为输入神经元的个数（例如你的样本特征为 5，则 $i=5$），$o$ 为输出神经元的个数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.9150], grad_fn=<AddBackward0>)\n",
      "____________________________________\n",
      "Parameter containing:\n",
      "tensor([[-0.2738, -0.6235]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3942], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = nn.Linear(2, 1) # 输入特征数为2，输出特征数为1\n",
    "\n",
    "linear_input = torch.Tensor([1, 2]) # 给一个样本，该样本有2个特征（这两个特征的值分别为1和2）\n",
    "linear_output = linear_layer(linear_input)\n",
    "print(linear_output)\n",
    "print(\"____________________________________\")\n",
    "# 查看模型参数\n",
    "# 模型有3个参数，分别为两个权重和一个bias\n",
    "for linear_param in linear_layer.parameters():\n",
    "    print(linear_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.4976, -0.0667],\n",
      "          [-0.6202,  0.0359],\n",
      "          [-0.7428,  0.1385]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# X每一行为一个特征\n",
    "\n",
    "X = torch.Tensor([\n",
    "    [0.1,0.2,0.3,0.3,0.3],\n",
    "    [0.4,0.5,0.6,0.6,0.6],\n",
    "    [0.7,0.8,0.9,0.9,0.9],\n",
    "])\n",
    "\n",
    "X = torch.reshape(X,(1,1,3,5))\n",
    "\n",
    "\n",
    "linear_layer_2 = nn.Linear(in_features=5, out_features=2, bias=True)\n",
    "\n",
    "print(linear_layer_2(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. `nn.Flatten()`\n",
    "\n",
    "`torch.nn.Flatten(start_dim=1, end_dim=- 1)`\n",
    "\n",
    "- 作用：将连续的维度范围展平为张量。 经常在`nn.Sequential()`中出现，一般写在某个神经网络模型之后，用于对神经网络模型的输出进行处理，得到`tensor`类型的数据。\n",
    "- `start_dim`和`end_dim`，分别表示开始的维度和终止的维度，默认值分别是1和-1，其中1表示第一维度，-1表示最后的维度。结合起来看意思就是从第一维度到最后一个维度全部给展平为张量。（注意：数据的维度是从0开始的，也就是存在第0维度，第一维度并不是真正意义上的第一个）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(32, 2, 5, 5)\n",
    "# With default parameters\n",
    "f = nn.Flatten()\n",
    "output = f(input)\n",
    "output.size()\n",
    "#torch.Size([32, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `nn.Sequential()`\n",
    "\n",
    "`nn.Sequential()` 是一个序列容器，用于搭建神经网络的模块被按照被传入器构造的顺序添加到容器中。除此之外，一个包含神经网络模块的`OrderedDict`也可以被传入`nn.Sequential()`容器中。利用`nn.Sequential()`搭建好模型架构，模型前向传播时调用`forward()`方法，模型接收的输入首先被传入`nn.Sequential()`包含的第一个网络模块中。然后，第一个网络模块的输出传入第二个网络模块作为输入，按照顺序依次计算并传播，直到`nn.Sequential()`里的最后一个模块输出结果。<p>\n",
    "-------------------------------------------------------------------------------------------------------- <p>\n",
    "与一层一层的单独调用模块组成序列相比，`nn.Sequential()` 可以允许将整个容器视为单个模块（即相当于把多个模块封装成一个模块），`forward()`方法接收输入之后，`nn.Sequential()`按照内部模块的顺序自动依次计算并输出结果。这就意味着我们可以利用`nn.Sequential() `自定义自己的网络层。\n",
    "\n",
    "```python\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channel, in_channel, kernel_size),\n",
    "                                    nn.BatchNorm2d(in_channel),\n",
    "                                    nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(in_channel, in_channel),\n",
    "                                    nn.BatchNorm2d(in_channel),\n",
    "                                    nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(in_channel, out_channel, kernel_size),\n",
    "                                    nn.BatchNorm2d(out_channel),\n",
    "                                    nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 批标准化层\n",
    "\n",
    "ref: [Batch Normalization（BN）超详细解析](https://blog.csdn.net/weixin_44023658/article/details/105844861)\n",
    "\n",
    "<p>\n",
    "\n",
    "对输入采用 `Batch Normalization`, 可以加快神经网络的训练速度 (在原paper中, BN被建议插入在 **(每个)** ReLU激活层前面).\n",
    "\n",
    "`CLASS torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)`\n",
    "\n",
    "- `num_features`：输入数据的通道数, C from an expected input of size (N,C,H,W)\n",
    "- `eps`：分母中添加的一个值，防止分母为0\n",
    "- `momentum`：用于计算运行时的均值和方差的动量\n",
    "- `affine`：当设为True时，会给该层添加可学习的缩放和平移参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2246,  0.1058],\n",
      "          [-1.0115,  0.1297]],\n",
      "\n",
      "         [[ 0.3059, -0.7004],\n",
      "          [ 1.4057, -0.6412]],\n",
      "\n",
      "         [[ 0.2998, -0.7642],\n",
      "          [-0.4625,  0.7384]]]])\n",
      "_____________________________________\n",
      "tensor([[[[ 0.7158,  0.4811],\n",
      "          [-1.7253,  0.5284]],\n",
      "\n",
      "         [[ 0.2490, -0.9253],\n",
      "          [ 1.5325, -0.8562]],\n",
      "\n",
      "         [[ 0.5814, -1.2018],\n",
      "          [-0.6961,  1.3165]]]])\n"
     ]
    }
   ],
   "source": [
    "# With Learnable Parameters\n",
    "m = nn.BatchNorm2d(3)\n",
    "# Without Learnable Parameters\n",
    "m = nn.BatchNorm2d(3, affine=False)\n",
    "input = torch.randn(1, 3, 2, 2)\n",
    "print(input)\n",
    "print(\"_____________________________________\")\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dropout Layer\n",
    "\n",
    "- 在训练过程中, 随机把一些 input(输入的tensor数据类型) 中的一些元素变为0, 变为0的概率为p.\n",
    "\n",
    "\n",
    "`torch.nn.Dropout(p=0.5, inplace=False)`\n",
    "\n",
    "- `p`：每个元素被置零的概率，默认值为0.5\n",
    "- `inplace`：如果设置为True，将会原地操作，即不会拷贝输入数据，会节省内存，但会破坏输入数据，因此默认为False\n",
    "\n",
    "```python\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "input = torch.randn(2, 3, 5, 5)\n",
    "output = dropout(input)\n",
    "print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recurrent Layers & Transformer Layers\n",
    "\n",
    "See [PyTorch Doc](https://pytorch.org/docs/stable/nn.html#recurrent-layers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. `nn Loss Functions`\n",
    "\n",
    "- `nn.L1Loss`\n",
    "- `nn.MSELoss`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential 的使用\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    " \n",
    "# Using Sequential with OrderedDict. This is functionally the\n",
    "# same as the above code\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：<p>\n",
    "\n",
    "假设输入 x 的尺寸是 (batch_size, 1, 32, 32)。<p>\n",
    "经过第一个卷积层 self.conv1(x)，输出特征图的尺寸为 (batch_size, 16, 28, 28)。<p>\n",
    "然后应用 ReLU 激活函数 F.relu(self.conv1(x))，不会改变特征图的尺寸，仍然是 (batch_size, 16, 28, 28)。<p>\n",
    "最后执行最大池化操作 F.max_pool2d(F.relu(self.conv1(x)), (2, 2))。<p>\n",
    "- 池化窗口大小为 (2, 2)，也就是高度和宽度都是 2。\n",
    "- 池化操作会将特征图的高度和宽度各缩小一半。\n",
    "\n",
    "因此，经过最大池化后，输出特征图的尺寸为 (batch_size, 16, 14, 14)。<p>\n",
    "综上所述，输入 x 的尺寸为 (batch_size, 1, 32, 32)，经过第一个卷积层和最大池化层后，输出特征图的尺寸变为 (batch_size, 16, 14, 14)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 输入是一个 32 x 32 的矩阵（图像）\n",
    "\n",
    "class Net(nn.Module):\n",
    "# 我们定义了一个名为 Net 的类,它继承自 nn.Module。这个类定义了一个简单的卷积神经网络(CNN)模型\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 在 __init__ 方法中,我们定义了该模型的各个层.\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 第一层卷积：1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        # self.fc1、self.fc2 和 self.fc3 是三个全连接层,分别将 16*5*5 维的输入映射到 120 维、120 维到 84 维,最后到 10 维(对应 10 个类别)。\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # 计算输入张量 x 展平后的特征数量\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window\n",
    "        \n",
    "        # If the pooling size is a square you can only specify a single number\n",
    "        # 如果池化窗口的高度和宽度是相同的,那么可以只指定一个数字,PyTorch 会自动将其扩展为 (2, 2) 这样的二元元组。\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x)) # 我们将二维的特征图 x 展平成一维向量\n",
    "        \n",
    "        # 接下来,我们将展平后的特征向量传递到三个全连接层 self.fc1, self.fc2 和 self.fc3\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型中必须要定义 ``forward`` 函数，``backward``\n",
    "函数（用来计算梯度）会被``autograd``自动创建。\n",
    "可以在 ``forward`` 函数中使用任何针对 Tensor 的操作。\n",
    "\n",
    " ``net.parameters()``返回可被学习的参数（权重）列表和值\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试随机输入32×32。\n",
    "注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0055, -0.0183,  0.0429, -0.0562,  0.0609, -0.0518,  0.0226,  0.0052,\n",
      "          0.0502,  0.0509]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``torch.nn`` 只支持小批量输入。整个 ``torch.nn``包都只支持小批量样本，而不支持单个样本。\n",
    "\n",
    "例如，``nn.Conv2d`` 接受一个4维的张量，每一维分别是``sSamples * nChannels * Height * Width（样本数*通道数*高*宽）``。\n",
    "<p>\n",
    "\n",
    "如果你有单个样本，只需使用 ``input.unsqueeze(0)`` 来添加其它的维数<p>\n",
    "\n",
    "在继续之前，我们回顾一下到目前为止用到的类。\n",
    "\n",
    "**回顾:**\n",
    "  -  ``torch.Tensor``：一个自动调用 ``backward()``实现支持自动梯度计算的 *多维数组* ，并且保存关于这个向量的*梯度* .\n",
    "  -  ``nn.Module``：神经网络模块。封装参数、移动到GPU上运行、导出、加载等。\n",
    "  -  ``nn.Parameter``：一种变量，当把它赋值给一个``Module``时，被 *自动* 地注册为一个参数。\n",
    "  -  ``autograd.Function``：实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个``Tensor``的操作都回创建一个接到创建``Tensor``和 *编码其历史* 的函数的``Function``节点。\n",
    "\n",
    "**重点如下：**\n",
    "  -  定义一个网络\n",
    "  -  处理输入，调用backword\n",
    "\n",
    "**还剩：**\n",
    "  -  计算损失\n",
    "  -  更新网络权重\n",
    "\n",
    "损失函数\n",
    "-------------\n",
    "一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。\n",
    "\n",
    "***译者注：output为网络的输出，target为实际值***\n",
    "\n",
    "nn包中有很多不同的[损失函数](https://pytorch.org/docs/nn.html#loss-functions)。\n",
    "``nn.MSELoss``是一个比较简单的损失函数，它计算输出和目标间的**均方误差**，\n",
    "例如：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6377, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # 随机值作为样例\n",
    "target = target.view(1, -1)  # 使target和output的shape相同\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，如果在反向过程中跟随``loss`` ， 使用它的\n",
    "``.grad_fn`` 属性，将看到如下所示的计算图。\n",
    "\n",
    "::\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "所以，当我们调用 ``loss.backward()``时,整张计算图都会\n",
    "根据loss进行微分，而且图中所有设置为``requires_grad=True``的张量\n",
    "将会拥有一个随着梯度累积的``.grad`` 张量。\n",
    "\n",
    "为了说明，让我们向后退几步:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x0000019FFFC67D30>\n",
      "<AddmmBackward0 object at 0x0000019F9111CD30>\n",
      "<AccumulateGrad object at 0x0000019F910F5D60>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播\n",
    "--------\n",
    "调用loss.backward()获得反向传播的误差。\n",
    "\n",
    "但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。\n",
    "\n",
    "现在，我们将调用loss.backward()，并查看conv1层的偏差（bias）项在反向传播前后的梯度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0051,  0.0042,  0.0026,  0.0152, -0.0040, -0.0036])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # 清除梯度\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何使用损失函数\n",
    "\n",
    "**稍后阅读：**\n",
    "\n",
    "  `nn`包，包含了各种用来构成深度神经网络构建块的模块和损失函数，完整的文档请查看[here](https://pytorch.org/docs/nn)。\n",
    "\n",
    "**剩下的最后一件事:**\n",
    "\n",
    "  - 新网络的权重\n",
    "\n",
    "更新权重\n",
    "------------------\n",
    "在实践中最简单的权重更新规则是随机梯度下降（SGD）： ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "我们可以使用简单的Python代码实现这个规则：\n",
    "\n",
    "```python\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```\n",
    "但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包``torch.optim``实现了所有的这些规则。\n",
    "使用它们非常简单：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. 注意::\n",
    "    \n",
    "观察如何使用``optimizer.zero_grad()``手动将梯度缓冲区设置为零。这是因为梯度是按Backprop部分中的说明累积的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "- [[PyTorch学习笔记]17：2D卷积,nn.Conv2d和F.conv2d](https://blog.csdn.net/SHU15121856/article/details/88956545)\n",
    "- [pytorch中nn.Sequential详解](https://blog.csdn.net/lsb2002/article/details/135096093)\n",
    "- [Pytorch nn.Linear()的基本用法与原理详解及全连接层简介](https://blog.csdn.net/qq_44722189/article/details/135035351)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
