{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac80476",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "Linear models make a prediction using a linear function of the input features $X$ \n",
    "\n",
    "$$f_{\\mathbf{w}}(\\mathbf{x}) = \\sum_{i=1}^{p} w_i \\cdot x_i + w_{0}$$\n",
    "\n",
    "Learn $w$ from $X$, given a loss function $\\mathcal{L}$:\n",
    "\n",
    "$$\\underset{\\mathbf{w}}{\\operatorname{argmin}} \\mathcal{L}(f_\\mathbf{w}(X))$$\n",
    "\n",
    "* Many algorithms with different $\\mathcal{L}$: Least squares, Ridge, Lasso, Logistic Regression, Linear SVMs,...\n",
    "* Can be very powerful (and fast), especially for large datasets with many features.\n",
    "* Can be generalized to learn non-linear patterns: _Generalized Linear Models_\n",
    "    * Features can be augmentented with polynomials of the original features\n",
    "    * Features can be transformed according to a distribution (Poisson, Tweedie, Gamma,...)\n",
    "    * Some linear models (e.g. SVMs) can be _kernelized_ to learn non-linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b86304",
   "metadata": {},
   "source": [
    "## 1. Linear models for regression\n",
    "\n",
    "### 1.1. Linear Regression (aka Ordinary Least Squares)\n",
    "* Loss function is the _sum of squared errors_ (SSE) (or residuals) between predictions $\\hat{y}_i$ (red) and the true regression targets $y_i$ (blue) on the training set.\n",
    "\n",
    "$$\\mathcal{L}_{SSE} = \\sum_{n=1}^{N} (y_n-\\hat{y}_n)^2 = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2$$ \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_least_squares.png\" alt=\"ml\" style=\"margin: 0 auto; width: 400px;\"/>\n",
    "\n",
    "#### 1.1.1. Solving ordinary least squares\n",
    "* Convex optimization problem with unique closed-form solution:\n",
    "    \n",
    "    $$w^{*} = (X^{T}X)^{-1} X^T Y$$\n",
    "    \n",
    "    * Add a column of 1's to the front of X to get $w_0$\n",
    "    * Slow. Time complexity is quadratic in number of features: $\\mathcal{O}(p^2n)$\n",
    "        * X has $n$ rows, $p$ features, hence $X^{T}X$ has dimensionality $p \\cdot p$\n",
    "    * **Only works if $n>p$**\n",
    "* **Very easily overfits**.\n",
    "    * coefficients $w$ become very large (steep incline/decline)\n",
    "    * small change in the input *x* results in a very different output *y* \n",
    "    * No hyperparameters that control model complexity\n",
    "\n",
    "#### 1.1.2. Gradient Descent\n",
    "* Start with an initial, random set of weights: $\\mathbf{w}^0$\n",
    "* Given a differentiable loss function $\\mathcal{L}$ (e.g. $\\mathcal{L}_{SSE}$), compute $\\nabla \\mathcal{L}$\n",
    "* For least squares: $\\frac{\\partial \\mathcal{L}_{SSE}}{\\partial w_i}(\\mathbf{w}) = -2\\sum_{n=1}^{N} (y_n-\\hat{y}_n) x_{n,i}$\n",
    "    * If feature $X_{:,i}$ is associated with big errors, the gradient wrt $w_i$ will be large\n",
    "* Update _all_ weights slightly (by _step size_ or _learning rate_ $\\eta$) in 'downhill' direction.\n",
    "* Basic _update rule_ (step s): \n",
    "\n",
    "    $$\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L}(\\mathbf{w}^s)$$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_gradient_descent.jpg\" alt=\"ml\" style=\"width: 400px;\"/>\n",
    "\n",
    "* Important hyperparameters\n",
    "    * Learning rate\n",
    "        * Too small: slow convergence. Too large: possible divergence\n",
    "    * Maximum number of iterations\n",
    "        * Too small: no convergence. Too large: wastes resources\n",
    "    * Learning rate decay with decay rate $k$\n",
    "        * E.g. exponential ($\\eta^{s+1} = \\eta^{0}  e^{-ks}$), inverse-time ($\\eta^{s+1} = \\frac{\\eta^{s}}{1+ks}$),...\n",
    "    * Many more advanced ways to control learning rate (see later)\n",
    "        * Adaptive techniques: depend on how much loss improved in previous step\n",
    "\n",
    "#### 1.1.3. Stochastic Gradient Descent (SGD)\n",
    "* Compute gradients not on the entire dataset, but on a single data point $i$ at a time\n",
    "    * Gradient descent: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L}(\\mathbf{w}^s) = \\mathbf{w}^s-\\frac{\\eta}{n} \\sum_{i=1}^{n} \\nabla \\mathcal{L_i}(\\mathbf{w}^s)$\n",
    "    * Stochastic Gradient Descent: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L_i}(\\mathbf{w}^s)$\n",
    "* Many smoother variants, e.g.\n",
    "    * Minibatch SGD: compute gradient on batches of data: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\frac{\\eta}{B} \\sum_{i=1}^{B} \\nabla \\mathcal{L_i}(\\mathbf{w}^s)$\n",
    "    * Stochastic Average Gradient Descent ([SAG](https://link.springer.com/content/pdf/10.1007/s10107-016-1030-6.pdf), [SAGA](https://proceedings.neurips.cc/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf)). With $i_s \\in [1,n]$ randomly chosen per iteration:\n",
    "        * Incremental gradient: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\frac{\\eta}{n} \\sum_{i=1}^{n} v_i^s$ with $v_i^s = \\begin{cases}\\nabla \\mathcal{L_i}(\\mathbf{w}^s) & i = i_s \\\\ v_i^{s-1} & \\text{otherwise} \\end{cases}$\n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_SGD.png\" alt=\"ml\" style=\"float: left; width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1629a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model = SGDRegressor(loss=\"squared_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117984d",
   "metadata": {},
   "source": [
    "### 1.2. Ridge regression\n",
    "\n",
    "* Adds a penalty term to the least squares loss function:\n",
    "\n",
    "$$\\mathcal{L}_{Ridge} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=1}^{p} w_i^2$$ \n",
    "\n",
    "* Model is penalized if it uses large coefficients ($w$)\n",
    "    * Each feature should have as little effect on the outcome as possible \n",
    "    * We don't want to penalize $w_0$, so we leave it out\n",
    "* Regularization: explicitly restrict a model to avoid overfitting. \n",
    "    * Called L2 regularization because it uses the L2 norm: $\\sum w_i^2$\n",
    "* The strength of the regularization can be controlled with the $\\alpha$ hyperparameter.\n",
    "    * Increasing $\\alpha$ causes more regularization (or shrinkage). Default is 1.0.\n",
    "- Still convex. Can be optimized in different ways:\n",
    "    - **Closed form solution** (a.k.a. Cholesky): $w^{*} = (X^{T}X + \\alpha I)^{-1} X^T Y$\n",
    "    - Gradient descent and variants, e.g. Stochastic Average Gradient (SAG,SAGA)\n",
    "        - Conjugate gradient (CG): each new gradient is influenced by previous ones\n",
    "    - **Use Cholesky for smaller datasets, Gradient descent for larger ones**\n",
    "\n",
    "``` python\n",
    "from sklearn.linear_model import Ridge\n",
    "lr = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d40d0",
   "metadata": {},
   "source": [
    "### 1.3. Lasso (Least Absolute Shrinkage and Selection Operator)\n",
    "* Adds a different penalty term to the least squares sum:\n",
    "\n",
    "$$\\mathcal{L}_{Lasso} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=1}^{p} |w_i|$$\n",
    "\n",
    "- Called L1 regularization because it uses the L1 norm, 可以用来做**特征选择/解决自变量间的多重共线性问题(变量之间存在强相关性, e.g. x_1 = 2x_2)**.\n",
    "    - **Will cause many weights to be exactly 0(sparse models)**\n",
    "    - Some features are ignored entirely: automatic feature selection\n",
    "- Same parameter $\\alpha$ to control the strength of regularization. \n",
    "    * Will again have a 'sweet spot' depending on the data\n",
    "- **No closed-form solution**\n",
    "- Convex, but no longer strictly convex, and not differentiable\n",
    "    * Weights can be optimized using **_coordinate descent_**\n",
    "\n",
    "#### 1.3.1. Coordinate descent (坐标下降法)\n",
    "- Alternative for gradient descent, supports **non-differentiable convex loss functions** (e.g. $\\mathcal{L}_{Lasso}$)\n",
    "- In every iteration, optimize a single coordinate $w_i$ (find minimum in direction of $x_i$)\n",
    "    - Continue with another coordinate, using a selection rule (e.g. round robin)\n",
    "- Faster iterations. **No need to choose a step size (learning rate)**.\n",
    "- May converge more slowly. Can't be parallellized.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_cd.png\" alt=\"ml\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### 1.3.2. Coordinate descent with Lasso\n",
    "\n",
    "- Remember that $\\mathcal{L}_{Lasso} = \\mathcal{L}_{SSE} + \\alpha \\sum_{i=1}^{p} |w_i|$ \n",
    "- For one $w_i$: $\\mathcal{L}_{Lasso}(w_i) = \\mathcal{L}_{SSE}(w_i) + \\alpha |w_i|$\n",
    "- The L1 term is not differentiable but convex: we can compute the [_subgradient_](https://towardsdatascience.com/unboxing-lasso-regularization-with-proximal-gradient-method-ista-iterative-soft-thresholding-b0797f05f8ea) \n",
    "    - Unique at points where $\\mathcal{L}$ is differentiable, a range of all possible slopes [a,b] where it is not\n",
    "    - For $|w_i|$, the subgradient $\\partial_{w_i} |w_i|$ =  $\\begin{cases}-1 & w_i<0\\\\ [-1,1] & w_i=0 \\\\ 1 & w_i>0 \\\\ \\end{cases}$\n",
    "\n",
    "    - Subdifferential $\\partial(f+g) = \\partial f + \\partial g$ if $f$ and $g$ are both convex\n",
    "- To find the optimum for Lasso $w_i^{*}$, solve\n",
    "\n",
    "    $$\\begin{aligned} \\partial_{w_i} \\mathcal{L}_{Lasso}(w_i) &= \\partial_{w_i} \\mathcal{L}_{SSE}(w_i) + \\partial_{w_i} \\alpha |w_i| \\\\ 0 &= (w_i - \\rho_i) + \\alpha \\cdot \\partial_{w_i} |w_i| \\\\ w_i &= \\rho_i - \\alpha \\cdot \\partial_{w_i} |w_i| \\end{aligned}$$\n",
    "\n",
    "    - In which $\\rho_i$ is the part of $\\partial_{w_i} \\mathcal{L}_{SSE}(w_i)$ excluding $w_i$ (assume $z_i=1$ for now)\n",
    "        - $\\rho_i$ can be seen as the $\\mathcal{L}_{SSE}$ 'solution': $w_i = \\rho_i$ if $\\partial_{w_i} \\mathcal{L}_{SSE}(w_i) = 0$ \n",
    "  $$\\partial_{w_i} \\mathcal{L}_{SSE}(w_i) = \\partial_{w_i} \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 = z_i w_i -\\rho_i $$ \n",
    "\n",
    "- We found: $w_i = \\rho_i - \\alpha \\cdot \\partial_{w_i} |w_i|$\n",
    "- [The Lasso solution](https://xavierbourretsicotte.github.io/lasso_derivation.html) has the form of a _soft thresholding function_ $S$\n",
    "\n",
    "    $$w_i^* = S(\\rho_i,\\alpha) = \\begin{cases} \\rho_i + \\alpha, & \\rho_i < -\\alpha \\\\  0, & -\\alpha < \\rho_i < \\alpha \\\\ \\rho_i - \\alpha, & \\rho_i > \\alpha \\\\ \\end{cases}$$\n",
    "    \n",
    "    - Small weights (all weights between $-\\alpha$ and $\\alpha$) **become 0: sparseness!**\n",
    "    - If the data is not normalized, $w_i^* = \\frac{1}{z_i}S(\\rho_i,\\alpha)$ with constant $z_i = \\sum_{n=1}^{N} x_{ni}^2$\n",
    "- Ridge solution: $w_i = \\rho_i - \\alpha \\cdot \\partial_{w_i} w_i^2 = \\rho_i - 2\\alpha \\cdot w_i$, thus $w_i^* = \\frac{\\rho_i}{1 + 2\\alpha}$\n",
    "\n",
    "- **参考1**: [凸优化笔记16：次梯度](https://zhuanlan.zhihu.com/p/139083837)\n",
    "- **参考2**: [LASSO的坐标下降法求解](https://zhuanlan.zhihu.com/p/429541451)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a7a5e",
   "metadata": {},
   "source": [
    "### 1.4. Elastic-Net\n",
    "\n",
    "* Adds both L1 and L2 regularization:\n",
    "\n",
    "$$\\mathcal{L}_{Elastic} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\rho \\sum_{i=1}^{p} |w_i| + \\alpha (1 -  \\rho) \\sum_{i=1}^{p} w_i^2$$ \n",
    "\n",
    "* $\\rho$ is the L1 ratio\n",
    "    * With $\\rho=1$, $\\mathcal{L}_{Elastic} = \\mathcal{L}_{Lasso}$\n",
    "    * With $\\rho=0$, $\\mathcal{L}_{Elastic} = \\mathcal{L}_{Ridge}$\n",
    "    * $0 < \\rho < 1$ sets a trade-off between L1 and L2.\n",
    "* Allows learning sparse models (like Lasso) while maintaining L2 regularization benefits\n",
    "    * E.g. if 2 features are correlated, Lasso likely picks one randomly, Elastic-Net keeps both \n",
    "* Weights can be optimized using coordinate descent (similar to Lasso) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694c556",
   "metadata": {},
   "source": [
    "### 1.5. Other Regression Loss\n",
    "\n",
    "- **Huber loss:** switches from squared loss to linear loss past a value $\\epsilon$\n",
    "    - More robust against **outliers**\n",
    "- **Epsilon insensitive:** ignores errors smaller than $\\epsilon$, and linear past that\n",
    "    - Aims to fit function so that residuals are at most $\\epsilon$\n",
    "    - **Also known as Support Vector Regression (`SVR` in sklearn)**\n",
    "- Squared Epsilon insensitive: ignores errors smaller than $\\epsilon$, and squared past that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825ba89",
   "metadata": {},
   "source": [
    "## 2. Linear models for Classification\n",
    "\n",
    "Aims to find a hyperplane that separates the examples of each class.  \n",
    "For binary classification (2 classes), we aim to fit the following function: \n",
    "\n",
    "$\\hat{y} = w_1 * x_1 + w_2 * x_2 +... + w_p * x_p + w_0 > 0$  \n",
    "    \n",
    "When $\\hat{y}<0$, predict class -1, otherwise predict class +1\n",
    "\n",
    "<p>\n",
    "\n",
    "There are many algorithms for linear classification, differing in loss function, regularization techniques, and optimization method\n",
    "\n",
    "* Most common techniques:\n",
    "    * Convert target classes {neg,pos} to {0,1} and treat as a regression task\n",
    "        * Logistic regression (Log loss)\n",
    "        * Ridge Classification (Least Squares + L2 loss)\n",
    "    * Find hyperplane that maximizes the margin between classes\n",
    "        * Linear Support Vector Machines (Hinge loss)\n",
    "    * Neural networks without activation functions\n",
    "        * Perceptron (Perceptron loss)\n",
    "    * SGDClassifier: can act like any of these by choosing loss function\n",
    "        * Hinge, Log, Modified_huber, Squared_hinge, Perceptron\n",
    "\n",
    "### 2.1. Logistic regression\n",
    "* Aims to predict the _probability_ that a point belongs to the positive class\n",
    "* Converts target values {negative, positive} to {0,1}\n",
    "* Fits a _logistic_ (or _sigmoid_ or _S_ curve) function through these points\n",
    "    * Maps (-Inf,Inf) to a probability [0,1]\n",
    "    \n",
    "    $$ \\hat{y} = \\textrm{logistic}(f_{\\theta}(\\mathbf{x})) = \\frac{1}{1+e^{-f_{\\theta}(\\mathbf{x})}} $$\n",
    "    \n",
    "* E.g. in 1D: $ \\textrm{logistic}(x_1w_1+w_0) = \\frac{1}{1+e^{-x_1w_1-w_0}} $\n",
    "\n",
    "#### 2.1.1. Loss function: Cross-entropy\n",
    "* Models that return class probabilities can use _cross-entropy loss_\n",
    "    \n",
    "    $$\\mathcal{L_{log}}(\\mathbf{w}) = \\sum_{n=1}^{N} H(p_n,q_n) = - \\sum_{n=1}^{N} \\sum_{c=1}^{C} p_{n,c} log(q_{n,c}) $$\n",
    "    \n",
    "    * Also known as log loss, logistic loss, or maximum likelihood\n",
    "    - Based on true probabilities $p$ (0 or 1) and predicted probabilities $q$ over $N$ instances and $C$ classes\n",
    "        - **Binary case (C=2)**: $\\mathcal{L_{log}}(\\mathbf{w}) = - \\sum_{n=1}^{N} \\big[ y_n log(\\hat{y}_n) + (1-y_n) log(1-\\hat{y}_n) \\big]$\n",
    "    * Penalty (a.k.a. 'surprise') grows exponentially as difference between $p$ and $q$ increases\n",
    "        * If you are sure of an answer (high $q$) and it's wrong (low $p$), you definitely want to learn\n",
    "    * Often used together with L2 (or L1) loss: $\\mathcal{L_{log}}'(\\mathbf{w}) = \\mathcal{L_{log}}(\\mathbf{w}) + \\alpha \\sum_{i} w_i^2 $\n",
    "\n",
    "#### 2.1.2. Optimization methods (solvers) for cross-entropy loss\n",
    "* Gradient descent (only supports L2 regularization)\n",
    "    - Log loss is differentiable, so we can use (stochastic) gradient descent\n",
    "    - Variants thereof, e.g. Stochastic Average Gradient (SAG, SAGA)\n",
    "* Coordinate descent (supports both L1 and L2 regularization)\n",
    "    - Faster iteration, but may converge more slowly, has issues with saddlepoints\n",
    "    - Called `liblinear` in sklearn. **Can't run in parallel.**\n",
    "* Newton-Rhapson or Newton Conjugate Gradient (only L2):\n",
    "    - Uses the Hessian $H = \\big[\\frac{\\partial^2 \\mathcal{L}}{\\partial x_i \\partial x_j} \\big]$: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta H^{-1}(\\mathbf{w}^s) \\nabla \\mathcal{L}(\\mathbf{w}^s)$\n",
    "    - **Slow** for large datasets. **Works well if solution space is (near) convex**\n",
    "* Quasi-Newton methods (only L2)\n",
    "    - Approximate, faster to compute\n",
    "    - E.g. Limited-memory Broyden–Fletcher–Goldfarb–Shanno (`lbfgs`)\n",
    "        - Default in sklearn for Logistic Regression\n",
    "- Data scaling helps convergence, minimizes differences between solvers\n",
    "\n",
    "#### 2.1.3. In practice\n",
    "* Logistic regression can also be found in `sklearn.linear_model`.\n",
    "    * `C` hyperparameter is the _inverse_ regularization strength: $C=\\alpha^{-1}$\n",
    "    * `penalty`: type of regularization: L1, L2 (default), Elastic-Net, or None\n",
    "    * `solver`: newton-cg, lbfgs (default), liblinear, sag, saga\n",
    "* Increasing C: less regularization, tries to overfit individual points\n",
    "\n",
    "``` python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74d480",
   "metadata": {},
   "source": [
    "### 2.2. Support vector machines\n",
    "- Decision boundaries close to training points may generalize badly\n",
    "    - Very similar (nearby) test point are classified as the other class\n",
    "- Choose a boundary that is as far away from training points as possible\n",
    "- The __support vectors__ are the training samples closest to the hyperplane\n",
    "- The __margin__ is the distance between the separating hyperplane and the _support vectors_\n",
    "- Hence, our objective is to _maximize the margin_\n",
    "\n",
    "参考1: [DDA3020 SVM Notes](../DDA3020/L07_SVM.pdf)\n",
    "\n",
    "<p>\n",
    "\n",
    "参考2: [Lecture 2. Linear models](https://ml-course.github.io/master/notebooks/02%20-%20Linear%20Models.html#kernelization)\n",
    "### 2.3. Perceptron\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
