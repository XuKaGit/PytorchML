{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57743461",
   "metadata": {},
   "source": [
    "# RNN, Transformer & NLP\n",
    "\n",
    "- Basics: word embeddings\n",
    "\n",
    "    - Word2Vec, FastText, GloVe\n",
    "\n",
    "- Sequence-to-sequence and autoregressive models\n",
    "\n",
    "    - Classifying Names with a Character-Level RNN\n",
    "    - Generating Names with a Character-Level RNN\n",
    "\n",
    "- Self-attention and transformer models\n",
    "    - Translation with a Sequence to Sequence Network and Attention\n",
    "- Vision Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6076f46d",
   "metadata": {},
   "source": [
    "## 1. Basics\n",
    "\n",
    "### 1.1. Text preprocessing pipelines\n",
    "\n",
    "- **分词 (Tokenization)**: 将文本拆分成单词或\"词元\"(**tokens**) \n",
    "- **子词分词器 (Subword tokenizers)**: 以子词单位进行灵活切分, 如 \"unbelievability\" 可切成 un、believ、abil、ity\n",
    "- **词干提取 (Stemming)**: 对单词进行简单的还原到词干, 例如将 \"the meeting\" 变成 \"the meet\"\n",
    "- **词形还原 (Lemmatization)**: NLP-based reduction, e.g. distinguishes between nouns and verbs (区分名词和动词等词性)\n",
    "- **去除停用词 (Discard stop words)** :去掉如 \"the\"、\"an\" 等常见但无实际意义的词\n",
    "- **Useful libraries**: [nltk](https://www.nltk.org/), [spaCy](https://spacy.io/), [gensim](https://radimrehurek.com/gensim/), [HuggingFace tokenizers](https://huggingface.co/docs/tokenizers/index),...\n",
    "\n",
    "\n",
    "### 1.2. Bag of word representation\n",
    "- First, build a **vocabulary** of all occuring words. Maps every word to an index.\n",
    "- Represent each document as an $N$ dimensional vector (top-$N$ most frequent words)\n",
    "    - One-hot (sparse) encoding: 1 if the word occurs in the document\n",
    "- Destroys the order of the words in the text (hence, a 'bag' of words)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/bag_of_words.png\" alt=\"ml\" style=\"width: 60%\"/>\n",
    "\n",
    "#### 1.2.1. Neural networks on bag of words\n",
    "- We can build neural networks on bag-of-word vectors\n",
    "    - Do a one-hot-encoding with 10000 most frequent words\n",
    "    - Simple model with 2 dense layers, ReLU activation, dropout\n",
    "    - Using **IMDB dataset** of movie reviews (label is `'positive'` or `'negative'`)\n",
    "    \n",
    "``` python\n",
    "self.model = nn.Sequential(\n",
    "    nn.Linear(10000, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b32603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62353cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: Remote end closed connection\n",
      "[nltk_data]     without response\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\86188/nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\share\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\86188\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\envs\\dsml\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\dsml\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\86188/nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\share\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\86188\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m----> 6\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 加载IMDB数据集\u001b[39;00m\n\u001b[0;32m     10\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/IMDB\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\dsml\\lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\dsml\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\dsml\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\dsml\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\86188/nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\share\\\\nltk_data'\n    - 'd:\\\\Anaconda\\\\envs\\\\dsml\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\86188\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# 使用nltk库删去停用词\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# 加载IMDB数据集\n",
    "data_dir = '../Data/IMDB'\n",
    "\n",
    "def read_imdb(data_dir, set_type='train'):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_type in ['pos', 'neg']:\n",
    "        label = 1 if label_type == 'pos' else 0\n",
    "        dir_path = os.path.join(data_dir, set_type, label_type)  # 拼接路径\n",
    "        for filename in os.listdir(dir_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    texts.append(text)\n",
    "                    labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "train_data, train_label = read_imdb(set_type='train')\n",
    "test_data, test_label = read_imdb(set_type='test')\n",
    "\n",
    "# 构建词表\n",
    "\n",
    "def build_vocab(max_vocab_size=None, train_text = train_data):\n",
    "    counter = Counter()    # 计数器的类, 用于记录各种单词的出现次数\n",
    "    for text in train_text:\n",
    "        text = text.lower()  # 转换为小写\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # 去除标点符号\n",
    "        tokens = text.split()  # 分词   \n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        counter.update(filtered_tokens)  # 计数(出现频率)\n",
    "    voc = [word for word, _ in counter.most_common(max_vocab_size)]  # 提取出现频率最高的\n",
    "    return voc\n",
    "\n",
    "\n",
    "vocab = build_vocab(max_vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58dc4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with top 10,000 words\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Vectorize sequences into one-hot encoded vectors\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension), dtype=np.float32)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0\n",
    "    return results\n",
    "\n",
    "# One-hot encode\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "class IMDBVectorizedDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.x = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "# Validation split like in Keras: first 10k for val\n",
    "x_val, x_partial_train = x_train[:10000], x_train[10000:]\n",
    "y_val, y_partial_train = y_train[:10000], y_train[10000:]\n",
    "\n",
    "train_dataset = IMDBVectorizedDataset(x_partial_train, y_partial_train)\n",
    "val_dataset = IMDBVectorizedDataset(x_val, y_val)\n",
    "test_dataset = IMDBVectorizedDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)\n",
    "\n",
    "class LivePlotCallback(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.train_accs = []\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "        self.max_acc = 0\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "\n",
    "        train_loss = metrics.get(\"train_loss\")\n",
    "        train_acc = metrics.get(\"train_acc\")\n",
    "        val_loss = metrics.get(\"val_loss\")\n",
    "        val_acc = metrics.get(\"val_acc\")\n",
    "\n",
    "        if all(v is not None for v in [train_loss, train_acc, val_loss, val_acc]):\n",
    "            self.train_losses.append(train_loss.item())\n",
    "            self.train_accs.append(train_acc.item())\n",
    "            self.val_losses.append(val_loss.item())\n",
    "            self.val_accs.append(val_acc.item())\n",
    "            self.max_acc = max(self.max_acc, val_acc.item())\n",
    "\n",
    "            if len(self.train_losses) > 1:\n",
    "                clear_output(wait=True)\n",
    "                N = np.arange(0, len(self.train_losses))\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                plt.plot(N, self.train_losses, label='train_loss', lw=2, c='r')\n",
    "                plt.plot(N, self.train_accs, label='train_acc', lw=2, c='b')\n",
    "                plt.plot(N, self.val_losses, label='val_loss', lw=2, linestyle=\":\", c='r')\n",
    "                plt.plot(N, self.val_accs, label='val_acc', lw=2, linestyle=\":\", c='b')\n",
    "                plt.title(f\"Training Loss and Accuracy [Max Val Acc: {self.max_acc:.4f}]\", fontsize=12)\n",
    "                plt.xlabel(\"Epoch\", fontsize=12)\n",
    "                plt.ylabel(\"Loss / Accuracy\", fontsize=12)\n",
    "                plt.tick_params(axis='both', labelsize=12)\n",
    "                plt.legend(fontsize=12)\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "            \n",
    "class IMDBClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10000, 16)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        acc = ((y_hat > 0.5) == y.bool()).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.binary_cross_entropy(y_hat, y)\n",
    "        val_acc = ((y_hat > 0.5) == y.bool()).float().mean()\n",
    "        self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", val_acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters())\n",
    "    \n",
    "model = IMDBClassifier()\n",
    "trainer = pl.Trainer(max_epochs=15, callbacks=[LivePlotCallback()], logger=False, enable_checkpointing=False)\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86027282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e51416c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f90ef4b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
